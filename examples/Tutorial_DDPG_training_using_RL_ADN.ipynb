{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This script is a tutorial for using RL-ADN to train the DDPG Agent step by step. The 34 node environment is used.\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "omG74oZsWwOW"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "import environment and show the information"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "{'voltage_limits': [0.95, 1.05],\n 'algorithm': 'Laurent',\n 'battery_list': [11, 15, 26, 29, 33],\n 'year': 2020,\n 'month': 1,\n 'day': 1,\n 'train': True,\n 'state_pattern': 'default',\n 'network_info': {'vm_pu': 1.0,\n  's_base': 1000,\n  'bus_info_file': '../data_sources/network_data/node_34/Nodes_34.csv',\n  'branch_info_file': '../data_sources/network_data/node_34/Lines_34.csv'},\n 'time_series_data_path': '../data_sources/time_series_data/34_node_time_series.csv'}"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rl_adn.environments.env import PowerNetEnv,env_config\n",
    "import pandas as pd\n",
    "env_config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prepare the data for the environment"
   ],
   "metadata": {
    "collapsed": false,
    "id": "NT2oQotDWwOZ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scale: from 2020-07-17 to 2021-01-01\n",
      "Data time interval: 15 minutes\n",
      "Dataset loaded from ../power_network_rl/data_sources/time_series_data/34_node_time_series.csv\n",
      "Dataset dimensions: (16224, 69)\n",
      "Dataset contains the following types of data:\n",
      "Active power columns: ['active_power_node_1', 'active_power_node_2', 'active_power_node_3', 'active_power_node_4', 'active_power_node_5', 'active_power_node_6', 'active_power_node_7', 'active_power_node_8', 'active_power_node_9', 'active_power_node_10', 'active_power_node_11', 'active_power_node_12', 'active_power_node_13', 'active_power_node_14', 'active_power_node_15', 'active_power_node_16', 'active_power_node_17', 'active_power_node_18', 'active_power_node_19', 'active_power_node_20', 'active_power_node_21', 'active_power_node_22', 'active_power_node_23', 'active_power_node_24', 'active_power_node_25', 'active_power_node_26', 'active_power_node_27', 'active_power_node_28', 'active_power_node_29', 'active_power_node_30', 'active_power_node_31', 'active_power_node_32', 'active_power_node_33', 'active_power_node_34'] (Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33])\n",
      "Reactive power columns: [] (Indices: [])\n",
      "Renewable active power columns: ['renewable_active_power_node_1', 'renewable_active_power_node_2', 'renewable_active_power_node_3', 'renewable_active_power_node_4', 'renewable_active_power_node_5', 'renewable_active_power_node_6', 'renewable_active_power_node_7', 'renewable_active_power_node_8', 'renewable_active_power_node_9', 'renewable_active_power_node_10', 'renewable_active_power_node_11', 'renewable_active_power_node_12', 'renewable_active_power_node_13', 'renewable_active_power_node_14', 'renewable_active_power_node_15', 'renewable_active_power_node_16', 'renewable_active_power_node_17', 'renewable_active_power_node_18', 'renewable_active_power_node_19', 'renewable_active_power_node_20', 'renewable_active_power_node_21', 'renewable_active_power_node_22', 'renewable_active_power_node_23', 'renewable_active_power_node_24', 'renewable_active_power_node_25', 'renewable_active_power_node_26', 'renewable_active_power_node_27', 'renewable_active_power_node_28', 'renewable_active_power_node_29', 'renewable_active_power_node_30', 'renewable_active_power_node_31', 'renewable_active_power_node_32', 'renewable_active_power_node_33', 'renewable_active_power_node_34'] (Indices: [34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67])\n",
      "Renewable reactive power columns: [] (Indices: [])\n",
      "Price columns: ['price'] (Indices: [68])\n",
      "-104.4077\n",
      "-9.02\n"
     ]
    }
   ],
   "source": [
    "env_config['network_info']['bus_info_file']='../power_network_rl/data_sources/network_data/node_34/Nodes_34.csv'\n",
    "env_config['network_info']['branch_info_file']='../power_network_rl/data_sources/network_data/node_34/Lines_34.csv'\n",
    "env_config['network_info']['time_series_data_path']='../power_network_rl/data_sources/time_series_data/34_node_time_series.csv'\n",
    "env=PowerNetEnv(env_config)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Vi5cdrNdWwOZ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from power_network_rl.DRL_algorithms.Agent import AgentDDPG\n",
    "from power_network_rl.DRL_algorithms.utility import Config, ReplayBuffer, SumTree, build_mlp, get_episode_return, get_optim_param\n",
    "import time"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "dHJpYAQfWwOZ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Set parameters for the experiment\n",
    "`args` involved in the training process of a reinforcement learning agent, specifically using the DDPG (Deep Deterministic Policy Gradient) algorithm. This explanation assumes familiarity with basic reinforcement learning concepts and terminology.\n",
    "\n",
    "**Environment Arguments (`env_args`)**\n",
    "- `env_name`: 'PowerNetEnv' the name of the environment\n",
    "- `state_dim`: the state dimension in the environment\n",
    "- `action_dim`: the action dimension in the environment\n",
    "- `if_discrete`: Indicates if the action space is discrete (False for continuous action space).\n",
    "\n",
    "**General Training Arguments**\n",
    "- `agent_class`: Specifies the class of the agent, set to AgentDDPG.\n",
    "- `env_class`: The class of the environment, set to None.\n",
    "- `run_name`: Identifier for the training run, set as 'DDPG_test'.\n",
    "\n",
    "**Buffer Configuration**\n",
    "- `gamma`: Discount factor for future rewards, set to 0.99.\n",
    "- `target_step`: Target steps for the agent to achieve in the environment.\n",
    "- `warm_up`: Number of actions before training starts, set to 2000.\n",
    "- `buffer_size`: Size of the replay buffer, set to 400,00.\n",
    "- `repeat_times`: Number of training repeats, set to 1 (suitable for PER).\n",
    "- `batch_size`: Size of the training batch, set to 512.\n",
    "\n",
    "**Device Configuration**\n",
    "- `GPU_ID`: Identifier for the GPU, set to 0.\n",
    "- `gpu_id`: GPU ID for training, matching GPU_ID.\n",
    "- `num_workers`: Number of parallel workers, set to 4.\n",
    "- `random_seed`: Seed for random number generation, set to 521.\n",
    "\n",
    "**Agent Configuration**\n",
    "- `net_dims`: Dimensions of the neural network, set to (256, 256, 256).\n",
    "- `learning_rate`: Learning rate for training, set to 6e-5.\n",
    "- `num_episode`: Number of training episodes, set to 1000 as an example.\n",
    "\n",
    "**Initialization and Execution**\n",
    "- `init_before_training()`: Initializes components before training.\n",
    "- `print()`: Prints the current configuration.\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "5yr-En4kWwOa"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "| Arguments Remove cwd: ./DDPG/DDPG_test\n",
      "{'action_dim': 5,\n",
      " 'agent_class': <class 'power_network_rl.DRL_algorithms.Agent.AgentDDPG'>,\n",
      " 'batch_size': 512,\n",
      " 'buffer_size': 400000,\n",
      " 'clip_grad_norm': 3.0,\n",
      " 'cwd': './DDPG/DDPG_test',\n",
      " 'env_args': {'action_dim': 5,\n",
      "              'env_name': 'PowerNetEnv',\n",
      "              'if_discrete': False,\n",
      "              'max_step': 96,\n",
      "              'num_envs': 1,\n",
      "              'state_dim': 46},\n",
      " 'env_class': None,\n",
      " 'env_name': 'PowerNetEnv',\n",
      " 'gamma': 0.99,\n",
      " 'gpu_id': 0,\n",
      " 'if_discrete': False,\n",
      " 'if_off_policy': True,\n",
      " 'if_remove': True,\n",
      " 'if_use_per': False,\n",
      " 'learner_gpus': 0,\n",
      " 'learning_rate': 6e-05,\n",
      " 'max_step': 96,\n",
      " 'net_dims': (256, 256, 256),\n",
      " 'num_envs': 1,\n",
      " 'num_episode': 5,\n",
      " 'num_threads': 8,\n",
      " 'num_workers': 4,\n",
      " 'random_seed': 521,\n",
      " 'repeat_times': 1,\n",
      " 'reward_scale': 1,\n",
      " 'run_name': 'DDPG_test',\n",
      " 'soft_update_tau': 0.005,\n",
      " 'state_dim': 46,\n",
      " 'state_value_tau': 0,\n",
      " 'target_step': 1000,\n",
      " 'train': True,\n",
      " 'warm_up': 2000}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "source": [
    "env_args = {\n",
    "    'env_name': 'PowerNetEnv',\n",
    "    'state_dim': env.state_space.shape[0],\n",
    "    'action_dim': env.action_space.shape[0],\n",
    "    'if_discrete': False\n",
    "}\n",
    "args = Config(agent_class=AgentDDPG, env_class=None, env_args=env_args)  # see `Config` for explanation\n",
    "args.run_name='DDPG_test'\n",
    "'''init buffer configuration'''\n",
    "args.gamma = 0.99  # discount factor of future rewards\n",
    "args.target_step=1000\n",
    "args.warm_up=2000#\n",
    "args.buffer_size = int(4e5)  #\n",
    "args.repeat_times = 1\n",
    "args.batch_size=512\n",
    "'''init device'''\n",
    "GPU_ID=0\n",
    "args.gpu_id = GPU_ID\n",
    "args.num_workers = 4\n",
    "args.random_seed=521\n",
    "'''init agent configration'''\n",
    "args.net_dims=(256,256,256)\n",
    "args.learning_rate=6e-5\n",
    "args.num_episode=5# using 10 episodes as an example\n",
    "'''init before training'''\n",
    "args.init_before_training()\n",
    "'''print configuration'''\n",
    "args.print()\n",
    "\n",
    "'''init agent'''\n",
    "agent = args.agent_class(args.net_dims, args.state_dim, args.action_dim, gpu_id=args.gpu_id, args=args)\n",
    "'''init buffer '''\n",
    "if args.if_off_policy:\n",
    "    buffer = ReplayBuffer(\n",
    "        gpu_id=args.gpu_id,\n",
    "        num_seqs=args.num_envs,\n",
    "        max_size=args.buffer_size,\n",
    "        state_dim=args.state_dim,\n",
    "        action_dim=1 if args.if_discrete else args.action_dim,\n",
    "        if_use_per=args.if_use_per,\n",
    "        args=args,\n",
    "    )\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "KJaN-3q2WwOa",
    "outputId": "eb6c9ba9-e696-4d07-a2a1-c91dc5684177",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training loop\n",
    "- First warm up buffer\n",
    "- Then after warm up, to the set steps, start to update net\n",
    "- Make the test on training set and record the performance metric based on your requirements  \n",
    "- Update the buffer to collect new experiences\n",
    "- Save trained agent\n",
    "\n"
   ],
   "metadata": {
    "id": "fuugAeapbEmd"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "buffer:1000\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/power_network_rl/DRL_algorithms/utility.py:395: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  s_tensor = torch.as_tensor((state,), device=device, dtype=torch.float)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "the year:2020,month:12,day:21 is used for testing this episode\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "curren epsiode is 0, reward:-15.064297996410644,violation time of one day for all nodes:32,violation value is -0.03972565685330336,buffer_length: 2000\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "the year:2020,month:12,day:17 is used for testing this episode\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "curren epsiode is 1, reward:-12.769444952407284,violation time of one day for all nodes:29,violation value is -0.03215923733220635,buffer_length: 3000\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "the year:2020,month:10,day:13 is used for testing this episode\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "curren epsiode is 2, reward:-2.5809982828595284,violation time of one day for all nodes:0,violation value is 0.0,buffer_length: 4000\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "the year:2020,month:10,day:12 is used for testing this episode\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "curren epsiode is 3, reward:-4.3901180818163335,violation time of one day for all nodes:4,violation value is 0.0,buffer_length: 5000\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "the year:2020,month:10,day:11 is used for testing this episode\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "curren epsiode is 4, reward:-3.248826404102601,violation time of one day for all nodes:0,violation value is 0.0,buffer_length: 6000\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "./DDPG/DDPG_test/act_target.pth\n",
      "./DDPG/DDPG_test/cri_target.pth\n",
      "./DDPG/DDPG_test/cri.pth\n",
      "./DDPG/DDPG_test/cri_optimizer.pth\n",
      "./DDPG/DDPG_test/act.pth\n",
      "./DDPG/DDPG_test/act_optimizer.pth\n"
     ]
    }
   ],
   "source": [
    "# training loop '''train loop'''\n",
    "buffer_items = agent.explore_env(env, args.target_step, if_random=True)\n",
    "buffer.update(buffer_items)  # warm up for ReplayBuffer\n",
    "if args.train:\n",
    "    collect_data = True\n",
    "    while collect_data:\n",
    "        print(f'buffer:{buffer.cur_size}')\n",
    "        with torch.no_grad():\n",
    "            buffer_items = agent.explore_env(env, args.target_step, if_random=True)\n",
    "            buffer.update(buffer_items)\n",
    "        if buffer.cur_size >= args.warm_up:\n",
    "            collect_data = False\n",
    "    torch.set_grad_enabled(False)\n",
    "    for i_episode in range(args.num_episode):\n",
    "\n",
    "        torch.set_grad_enabled(True)\n",
    "        critic_loss, actor_loss, = agent.update_net(buffer)\n",
    "        torch.set_grad_enabled(False)\n",
    "        episode_reward, violation_time, violation_value, reward_for_power, reward_for_good_action, reward_for_penalty, state_list = get_episode_return(\n",
    "            env, agent.act,\n",
    "            agent.device)\n",
    "        print(\n",
    "            f'curren epsiode is {i_episode}, reward:{episode_reward},violation time of one day for all nodes:{violation_time},violation value is {violation_value},buffer_length: {buffer.cur_size}')\n",
    "        if i_episode % 1 == 0:\n",
    "            # target_step, continuly update replay buffer\n",
    "            buffer_items = agent.explore_env(env, args.target_step, if_random=False)\n",
    "            buffer.update(buffer_items)\n",
    "agent.save_or_load_agent(args.cwd, if_save=True)\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "qawyH1TXWwOa",
    "outputId": "25e0b1a0-9307-4d2d-e46c-d8f3068734ce",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Dk_QrJaXWwOa"
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}